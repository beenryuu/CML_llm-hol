{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Working With Pinecone Vector Database\n",
    "Although Vector Databases have existed long before Large Language Models, vector DBs have become an important part of many LLM solutions. In particular, Retreival Augmented Generation (or RAG) architecture addresses LLM's halucinations and issues with longer-term memory by augmenting the user's prompt with the results of a search accross a vector DB. [Pinecone](https://www.pinecone.io/) is a cloud-based vector database that is easy to integrate with your CML workflow, as this notebook shows. \n",
    "\n",
    "Recall that in the previous exervice you created CML jobs to scrape a site and load each page into Pinecone vector DB. This notebook will focus on interacting with Pinecone from a Jupyter notebook.\n",
    "\n",
    "![Exercise 3 overview](../assets/exercise_3.png)\n",
    "\n",
    "### 3.1 Imports and global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pinecone import Pinecone\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_ENVIRONMENT = os.getenv('PINECONE_ENVIRONMENT')\n",
    "PINECONE_INDEX = os.getenv('PINECONE_INDEX')\n",
    "dimension = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize Pinecone connection\n",
    "Pinecone client is initialized with the parameters defined previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising Pinecone connection...\n",
      "Pinecone initialised\n",
      "Getting 'llm-hol' as object...\n",
      "Success\n",
      "Total number of embeddings in Pinecone index is 15.\n"
     ]
    }
   ],
   "source": [
    "print(\"initialising Pinecone connection...\")\n",
    "pinecone = Pinecone(api_key=PINECONE_API_KEY)\n",
    "print(\"Pinecone initialised\")\n",
    "\n",
    "print(f\"Getting '{PINECONE_INDEX}' as object...\")\n",
    "index = pinecone.Index(PINECONE_INDEX)\n",
    "print(\"Success\")\n",
    "\n",
    "# Get latest statistics from index\n",
    "current_collection_stats = index.describe_index_stats()\n",
    "print('Total number of embeddings in Pinecone index is {}.'.format(current_collection_stats.get('total_vector_count')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Function to peform the vector search \n",
    "The idea is to find a chunk from the Knowledge Base that is \"close\" to what the original user's prompt is. We perform a semantic search using the user's question, find the nearest knowledge base chunk, and return the content of that chunk along with its source and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get embeddings for a user question and query Pinecone vector DB for nearest knowledge base chunk\n",
    "def get_nearest_chunk_from_pinecone_vectordb(index, question):\n",
    "    # Generate embedding for user question with embedding model\n",
    "    retriever = SentenceTransformer(EMBEDDING_MODEL_REPO)\n",
    "    xq = retriever.encode([question]).tolist()\n",
    "    xc = index.query(vector=xq, top_k=5,\n",
    "                 include_metadata=True)\n",
    "    \n",
    "    matching_files = []\n",
    "    scores = []\n",
    "    for match in xc['matches']:\n",
    "        # extract the 'file_path' within 'metadata'\n",
    "        file_path = match['metadata']['file_path']\n",
    "        # extract the individual scores for each vector\n",
    "        score = match['score']\n",
    "        scores.append(score)\n",
    "        matching_files.append(file_path)\n",
    "\n",
    "    # Return text of the nearest knowledge base chunk \n",
    "    # Note that this ONLY uses the first matching document for semantic search. matching_files holds the top results so you can increase this if desired.\n",
    "    response = load_context_chunk_from_data(matching_files[0])\n",
    "    sources = matching_files[0]\n",
    "    score = scores[0]\n",
    "    return response, sources, score\n",
    "\n",
    "# Return the Knowledge Base doc based on Knowledge Base ID (relative file path)\n",
    "def load_context_chunk_from_data(id_path):\n",
    "    with open(id_path, \"r\") as f: # Open file in read mode\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Examine the results of the vector search\n",
    "Given the text of the question, we can now perform a vector search and output the results in the notebook. An important detail here is the ability to interact with metadata (e.g. context source) which can be used to narrow down the search space and, more critically, for authorization. These approaches are out of scope of this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c945ded3baba4881967b7b3e4e9f8b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5a706e4ec94c408ffde4b96a2dc123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16038f997f984a92bbc4fcde36c3c3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61163f06f2ef4554badba636ad2ee3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b336c0d200e4aef971b620466f2f25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e6c8659b9b4ae394c227a3eb33ef72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef04a6a96494ed89fe30c24492e46d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11434cf5800f48e586b17b9aaa92700a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e5da6b534b41849e8bed88ac2ee174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb095c647b6d4888aaafcc1bbfe113d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba9534e7d934efe9bc39a555ac3a302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6e6c87f124469f83a47b00c9e83778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4be4a2725454781950c8b2f38617d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b87cdf059b4efab7c9db93b85cdd0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6075460d566f47d5ab28dd42e9ca9110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/cdsw/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/home/cdsw/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA SETUP: Loading binary /home/cdsw/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "2024-05-02 09:25:02.363858: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-02 09:25:03.120321: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-02 09:25:03.120358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-02 09:25:03.146000: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-02 09:25:03.227325: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 09:25:08.903803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context Chunk: \n",
      "ML RuntimesCloudera Docs ML Runtimes ML Runtimes are responsible for running data science workloads and intermediating       access to the underlying cluster.  CML allows you to run any code via an interactive session, scheduled job, or deployed model          or application. Data Scientists can use interactive sessions to explore data, or develop a          model. They can create jobs and schedule them to run at specified times or productionize          their work as a model to provide a REST endpoint or as an application that offers an          interactive data dashboard for business users. All of these workloads run inside an ML          Runtime container on top of Kubernetes.  Cloudera ML Runtimes are purpose built to serve a specific use-case. They are available          with a single editor (for example, Workbench, Jupyterlab), ship a single language Kernel          (for example, Python 3.8 or R 4.0), and have a set of UNIX tools and utilities or language          libraries and packages.  ML Runtimes have been open sourced and are available in the cloudera/ml-runtimes GitHub repository. If you need to understand your Runtime          environments fully or want to build a new Runtime from scratch,  you can access the          Dockerfiles that were used to build the ML Runtime container images in this repository. There is a wide range of supported Runtimes out-of-the-box that cover the large majority of          Data Science use-cases, but any special requirements can be satisfied by building a custom          ML Runtime container image. CML also supports quota management for CPU, GPU, and memory to limit the amount of          resources users have access to within the CML workspace.  Before ML Runtimes, CML offered similar functionalities via Legacy Engines. These          deprecated container images followed a monolithic architecture, all kernels (Python, R,          Scala), and all seemingly useful packages and libraries were included in the image.   Parent topic: Architecture Overview\n",
      "\n",
      "Context Source(s): \n",
      "/home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-runtimes.txt\n",
      "\n",
      "Pinecone Score: \n",
      "0.503948867\n"
     ]
    }
   ],
   "source": [
    "question = \"What is ML Runtime?\" ## (Swap with your own based on your dataset)\n",
    "\n",
    "context_chunk, sources, score = get_nearest_chunk_from_pinecone_vectordb(index, question)\n",
    "print(\"\\nContext Chunk: \")\n",
    "print(context_chunk)\n",
    "print(\"\\nContext Source(s): \")\n",
    "print(sources)\n",
    "print(\"\\nPinecone Score: \")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Takeaways\n",
    "* Vector search is a critical component of any LLM app using RAG architecture\n",
    "* Cloudera's partner [Pinecone](https://www.pinecone.io/) provides a convenient SaaS offering for a Vector Database to support LLM RAG architecture\n",
    "* Metadata from each entry in the Vector DB can be used to refine searches and add custom authorization frameworks\n",
    "\n",
    "### Up Next: Go to Exercise 4\n",
    "Where a gradio app is launched to complete the first iteration of the Q&A chat use case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
